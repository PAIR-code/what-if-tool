{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIT Toxicity Text Model Comparison",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNxsd4_q9wq",
        "colab_type": "text"
      },
      "source": [
        "### What-If Tool toxicity text model comparison\n",
        "\n",
        "Copyright 2019 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "This notebook shows use of the [What-If Tool](https://pair-code.github.io/what-if-tool) to compare two text models that determine sentence toxicity, one of which has had some debiasing performed during training.\n",
        "\n",
        "This notebook loads two pretrained toxicity models from [ConversationAI](https://github.com/conversationai/unintended-ml-bias-analysis) and compares them on the [wikipedia comments dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973).\n",
        "\n",
        "This notebook also shows how the What-If Tool can be used on non-TensorFlow models. In this case, these models are keras models that do not use tensorflow Examples as an input format. These models can be analyzed in the What-If Tool by supplying a custom prediction function to WitWidget.\n",
        "\n",
        "It also shows use of a user-provided custom distance function (for counterfactual analysis and datapoint similarity visualizations). The [tf.Hub Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2) is used to compute the similarity of the input text comments.\n",
        "\n",
        "##WARNING: Some text examples in this notebook include profanity, offensive statements, and offensive statements involving identity terms. Please feel free to avoid using this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqB2tjOMETmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Install the What-If Tool widget if running in colab {display-mode: \"form\"}\n",
        "\n",
        "# If running in colab then pip install, otherwise no need.\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade witwidget\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBOHfrOP7Iy5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download the pretrained keras model files\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_model.h5 -o ./cnn_wiki_tox_v3_model.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_hparams.h5 -o ./cnn_wiki_tox_v3_hparams.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_tokenizer.pkl -o ./cnn_wiki_tox_v3_tokenizer.pkl\n",
        "\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_model.h5 -o ./cnn_debias_tox_v3_model.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_hparams.h5 -o ./cnn_debias_tox_v3_hparams.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_tokenizer.pkl -o ./cnn_debias_tox_v3_tokenizer.pkl\n",
        "\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/wiki_test.csv -o ./wiki_test.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZR3i6UZlZ96",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load the keras models\n",
        "import sys\n",
        "from keras.models import load_model\n",
        "from six.moves import cPickle as pkl\n",
        "\n",
        "def pkl_load(f):\n",
        "  return pkl.load(f) if sys.version_info < (3, 0) else pkl.load(\n",
        "    f, encoding='latin1')\n",
        "\n",
        "model1 = load_model('cnn_wiki_tox_v3_model.h5')\n",
        "with open('cnn_wiki_tox_v3_tokenizer.pkl', 'rb') as f:\n",
        "  tokenizer1 = pkl_load(f)\n",
        "tokenizer1.oov_token = None # quick fix for version issues\n",
        "\n",
        "model2 = load_model('cnn_debias_tox_v3_model.h5')\n",
        "with open('cnn_debias_tox_v3_tokenizer.pkl', 'rb') as f:\n",
        "  tokenizer2 = pkl_load(f)\n",
        "tokenizer2.oov_token = None # quick fix for version issues"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nStoYhqT80WH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define custom prediction functions so that WIT infers using keras models\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set up model helper functions:\n",
        "PADDING_LEN = 250\n",
        "\n",
        "# Convert list of tf.Examples to list of comment strings.\n",
        "def examples_to_strings(examples):\n",
        "  texts = [ex.features.feature['comment'].bytes_list.value[0] for ex in examples]\n",
        "  if sys.version_info >= (3, 0):\n",
        "    texts = [t.decode('utf-8') for t in texts]\n",
        "  return texts\n",
        "\n",
        "# Get raw string out of tf.Example and prepare it for keras model input\n",
        "def examples_to_model_in(examples, tokenizer):\n",
        "  texts = examples_to_strings(examples)\n",
        "  # Tokenize string into fixed length sequence of integer based on tokenizer \n",
        "  # and model padding\n",
        "  text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "  model_ins = pad_sequences(text_sequences, maxlen=PADDING_LEN)\n",
        "  return model_ins\n",
        "\n",
        "# WIT predict functions:\n",
        "def custom_predict_1(examples_to_infer):\n",
        "  model_ins = examples_to_model_in(examples_to_infer, tokenizer1)\n",
        "  preds = model1.predict(model_ins)\n",
        "  return preds\n",
        "\n",
        "def custom_predict_2(examples_to_infer):\n",
        "  model_ins = examples_to_model_in(examples_to_infer, tokenizer2)\n",
        "  preds = model2.predict(model_ins)\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXaUORW0DVhg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Define helper functions for dataset conversion from csv to tf.Examples\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "  examples = []\n",
        "  if columns == None:\n",
        "    columns = df.columns.values.tolist()\n",
        "  for index, row in df.iterrows():\n",
        "    example = tf.train.Example()\n",
        "    for col in columns:\n",
        "      if df[col].dtype is np.dtype(np.int64):\n",
        "        example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "      elif df[col].dtype is np.dtype(np.float64):\n",
        "        example.features.feature[col].float_list.value.append(row[col])\n",
        "      elif row[col] == row[col]:\n",
        "        example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "    examples.append(example)\n",
        "  return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu398ARdeuxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Read the dataset from CSV and process it for model {display-mode: \"form\"}\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'wiki_test.csv'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = None\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
        "df = df[['is_toxic', 'comment']]\n",
        "\n",
        "# Remove non ascii characters\n",
        "comments = df['comment'].values\n",
        "proc_comments = []\n",
        "comment_lengths = []\n",
        "for c in comments:\n",
        "  try:\n",
        "    if sys.version_info >= (3, 0):\n",
        "      c = bytes(c, 'utf-8')\n",
        "    c = c.decode('unicode_escape')\n",
        "    if sys.version_info < (3, 0):\n",
        "      c = c.encode('ascii', 'ignore')\n",
        "    proc_comment = c.strip()\n",
        "  except:\n",
        "    proc_comment = ''\n",
        "  proc_comments.append(proc_comment)\n",
        "  comment_lengths.append(len(proc_comment.split()))\n",
        "\n",
        "df = df.assign(comment=proc_comments)\n",
        "df['comment length'] = comment_lengths\n",
        "\n",
        "label_column = 'is_toxic'\n",
        "make_label_column_numeric(df, label_column, lambda val: val)\n",
        "\n",
        "examples = df_to_examples(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVaMyc45HWwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Define a custom distance function for comparing datapoints (uses tf.Hub) {display-mode: \"form\"}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
        "session = tf.Session()\n",
        "session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "\n",
        "# For this use-case, we set distance between datapoints to be cosine distance\n",
        "# between unit-normalized embeddings of each datapoint from the tf.Hub\n",
        "# Universal Sentence Encoder.\n",
        "def universal_sentence_encoder_distance(input_example, examples_to_compare, _):\n",
        "  # Extract comment strings\n",
        "  input_sentence = examples_to_strings([input_example])[0]\n",
        "  sentences = examples_to_strings(examples_to_compare)\n",
        "\n",
        "  # Normalize all embeddings for cosine distance operation\n",
        "  input_emb = tf.squeeze(tf.nn.l2_normalize(embed([input_sentence]), axis=1))\n",
        "  sentences_emb = tf.nn.l2_normalize(embed(sentences), axis=1)\n",
        "\n",
        "  # Tile the input example for easy comparison to all examples\n",
        "  multiply = tf.constant([len(examples_to_compare)])\n",
        "  input_matrix = tf.reshape(tf.tile(input_emb, multiply),\n",
        "                            [multiply[0], tf.shape(input_emb)[0]])\n",
        "  \n",
        "  # Compute cosine distance from input example to all examples.\n",
        "  distances = tf.losses.cosine_distance(\n",
        "      sentences_emb, input_matrix, axis=1, reduction=tf.losses.Reduction.NONE)\n",
        "  results = session.run(tf.squeeze(distances))\n",
        "  return results.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwiWGrLlSWGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Invoke What-If Tool for the data and two models (Note that this step may take a while due to prediction speed of the toxicity model){display-mode: \"form\"}\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "num_datapoints = 1000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 720  #@param {type: \"number\"}\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(examples[:num_datapoints]).set_custom_predict_fn(\n",
        "  custom_predict_1).set_compare_custom_predict_fn(custom_predict_2).set_custom_distance_fn(\n",
        "      universal_sentence_encoder_distance)\n",
        "\n",
        "wv = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1s1_SiOyS0l",
        "colab_type": "text"
      },
      "source": [
        "#### Exploration ideas\n",
        "\n",
        "- Organize datapoints by setting X-axis scatter to \"inference score 1\" and Y-axis scatter to \"inference score 2\" to see how each datapoint differs in score between the original model (1) and debiased model (2). Points off the diagonal have differences in results between the two models.\n",
        "  - Are there patterns of which datapoints don't agree between the two models?\n",
        "  - If you set the ground truth feature dropdown in the \"Performance + Fairness\" tab to \"is_toxic\", then you can color or bin the datapoints by \"inference correct 1\" or \"inference correct 2\". Are there patterns of which datapoints are incorrect for model 1? For model 2? You may want to focus on terms listed [here](https://github.com/conversationai/unintended-ml-bias-analysis/blob/master/unintended_ml_bias/bias_madlibs_data/adjectives_people.txt)\n",
        "- Choose a datapoint and click on \"Show nearest counterfactual datapoint\", this will find another comment that is closest to the selected comment in terms of Universal Sentence Encoder embedding values, but has a different prediction (if selected comment is predicted to be \"toxic\" the counterfactual one will have a \"not toxic\" prediction).\n",
        "- With a selected datapoint of interest, use the \"Show similarity to selected datapoint\" option to plot datapoints by similarity (either as an axis in a scatterplot or as criteria for binning a histogram) to the one of interest, to explore performance on similar comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuZjEn5qOHFH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Add a feature column for each identity term to indicate if it exists in the comment\n",
        "!wget https://raw.githubusercontent.com/conversationai/unintended-ml-bias-analysis/master/unintended_ml_bias/bias_madlibs_data/adjectives_people.txt\n",
        "\n",
        "import re\n",
        "import six\n",
        "\n",
        "with open('adjectives_people.txt', 'r') as f:\n",
        "  segments = f.read().strip().split('\\n')\n",
        "print(segments)\n",
        "\n",
        "# Tag every sentence with an identity term\n",
        "comments = df['comment'].values\n",
        "seg_anns = {}\n",
        "selected_segments = segments\n",
        "for s in selected_segments:\n",
        "  is_seg = []\n",
        "  for c in comments:\n",
        "    if re.search(s, c):\n",
        "      is_seg.append(1)\n",
        "    else:\n",
        "      is_seg.append(0)\n",
        "  seg_anns[s] = is_seg\n",
        "\n",
        "for seg_key, seg_ann in six.iteritems(seg_anns):\n",
        "  df[seg_key] = pd.Series(seg_ann, index=df.index)\n",
        "\n",
        "examples = df_to_examples(df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}